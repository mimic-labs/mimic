{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arshs\\miniconda3\\envs\\dlproj\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME: c:\\Users\\arshs\\OneDrive\\Documents\\GitHub\\mimic\\segmentation\n",
      "c:\\Users\\arshs\\OneDrive\\Documents\\GitHub\\mimic\\segmentation\\FastSAM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Tuple\n",
    "from PIL import ImageDraw\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)\n",
    "%cd {HOME}/FastSAM\n",
    "\n",
    "from Inference import segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_point(videoPath, out=False):\n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False,\n",
    "                        max_num_hands=2,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    centers = [[], []]\n",
    "    wrists = [[], []]\n",
    "    finger_pts = [[], []]\n",
    "\n",
    "    # Function to calculate standard deviation of distances\n",
    "    def calculate_stdev(coordinates):\n",
    "        distances = np.linalg.norm(coordinates - np.mean(coordinates, axis=0), axis=1)\n",
    "        stdev = np.std(distances)\n",
    "        return stdev\n",
    "\n",
    "    # Function to draw a circle on the frame\n",
    "    def draw_circle(frame, center, RED=1):\n",
    "        cv2.circle(frame, center, 20, (0, 255*RED, 255*(1-RED)), -1)\n",
    "        \n",
    "    def extract_hand_data(frame):\n",
    "        results = hands.process(frame)\n",
    "        # print(results.multi_hand_world_landmarks)\n",
    "\n",
    "        if results.multi_hand_world_landmarks:\n",
    "            for hand in results.multi_handedness:\n",
    "                # Get a constant index for the detected hand (0 or 1). If only 1 hand is detected, default to index = 0.\n",
    "                hand_idx = hand.classification[0].index\n",
    "                try:\n",
    "                    hand_landmarks = results.multi_hand_landmarks[hand_idx]\n",
    "                except:\n",
    "                    hand_idx = 0\n",
    "                    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "                \n",
    "                # Get key points on palm\n",
    "                # palm_points = np.asarray([[hand_landmarks.landmark[12].x, hand_landmarks.landmark[12].y, hand_landmarks.landmark[12].z], \n",
    "                #                         [hand_landmarks.landmark[16].x, hand_landmarks.landmark[16].y, hand_landmarks.landmark[16].z], \n",
    "                #                         [hand_landmarks.landmark[20].x, hand_landmarks.landmark[20].y, hand_landmarks.landmark[20].z],\n",
    "                #                         [hand_landmarks.landmark[4].x, hand_landmarks.landmark[4].y, hand_landmarks.landmark[4].z],\n",
    "                #                         [hand_landmarks.landmark[8].x, hand_landmarks.landmark[8].y, hand_landmarks.landmark[8].z]])\n",
    "                palm_points = np.asarray([\n",
    "                        [hand_landmarks.landmark[4].x, hand_landmarks.landmark[4].y, hand_landmarks.landmark[4].z],\n",
    "                        [hand_landmarks.landmark[8].x, hand_landmarks.landmark[8].y, hand_landmarks.landmark[8].z]])\n",
    "\n",
    "                # Get palm orientation by calculating normal vector of palm plane\n",
    "                # normal_vector = np.cross(palm_points[2] - palm_points[0], palm_points[1] - palm_points[2])\n",
    "                # normal_vector /= np.linalg.norm(normal_vector)\n",
    "                # orientations[hand_idx].append(normal_vector)\n",
    "\n",
    "                # both contact points\n",
    "                finger_tips = np.copy(palm_points)\n",
    "                finger_tips[:, 0] *= frame.shape[1]\n",
    "                finger_tips[:, 1] *= frame.shape[0]\n",
    "                finger_pts[hand_idx].append(finger_tips)\n",
    "\n",
    "                # Get hand center\n",
    "                palm_points_mean = np.mean(palm_points, axis=0)\n",
    "                center_x = int(palm_points_mean[0] * frame.shape[1])\n",
    "                center_y = int(palm_points_mean[1] * frame.shape[0])\n",
    "                centers[hand_idx].append((center_x, center_y))\n",
    "            \n",
    "                wrist_center_x = int(hand_landmarks.landmark[0].x * frame.shape[1])\n",
    "                wrist_center_y = int(hand_landmarks.landmark[0].y * frame.shape[0])\n",
    "                wrists[hand_idx].append((wrist_center_x, wrist_center_y))\n",
    "                # cv2.circle(frame, (center_x, center_y), 3, (255, 0, 0))\n",
    "            # for x in range(2):\n",
    "            #     # Draw current & past hand centers on existing frame\n",
    "            #     cv2.polylines(frame, [np.array(centers[x])], False, (0,255*(1-x),255*x), 3)\n",
    "            \n",
    "        return frame\n",
    "\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    if out:\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(f\"out.mp4\"),\n",
    "            cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "            fps, (frame_width, frame_height)\n",
    "        )\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "\n",
    "    # Parameters\n",
    "    window_size = 20\n",
    "    stdev_threshold = 5\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    # Initialize variables\n",
    "    centers_window = [[],[]]\n",
    "\n",
    "    on = [False, False]\n",
    "    ret_frame_point = [] #frame, hand1_point, hand2_point\n",
    "    for x in tqdm(range(num_frames)):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get hand center coordinates\n",
    "        frame = extract_hand_data(frame)\n",
    "        frame_level = []\n",
    "        # print(centers)\n",
    "        for h in range(2):\n",
    "            hand_center = centers[h][-1] if len(centers[h]) > 0 else None\n",
    "            # print(hand_center)\n",
    "\n",
    "            if hand_center is not None:\n",
    "                # Add hand center to the window\n",
    "                centers_window[h].append(hand_center)\n",
    "\n",
    "                # Keep the window size limited to the last 20 frames\n",
    "                if len(centers_window[h]) > window_size:\n",
    "                    centers_window[h].pop(0)\n",
    "\n",
    "                # Calculate standard deviation of distances\n",
    "                if len(centers_window[h]) == window_size:\n",
    "                    stdev = calculate_stdev(np.array(centers_window[h]))\n",
    "\n",
    "                    # Check if stdev is below the threshold\n",
    "                    if stdev < stdev_threshold:\n",
    "                        if not on[h]:\n",
    "                            on[h]=True\n",
    "                            # Draw a circle on the frame at the average center\n",
    "                            average_center = tuple(centers_window[h][-1]) #tuple(np.mean(centers_window, axis=0).astype(int))\n",
    "                            frame_level.append((x,average_center, wrists[h][-1], finger_pts[h][-1]))\n",
    "                            draw_circle(frame, average_center, RED=h)\n",
    "                    else:\n",
    "                        on[h]=False\n",
    "        if frame_level:\n",
    "            ret_frame_point.append(frame_level)\n",
    "\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Display the frame\n",
    "        if out:\n",
    "            out.write(frame)\n",
    "\n",
    "        # Exit when 'q' key is pressed\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "\n",
    "    # Release video capture\n",
    "    if out:\n",
    "        out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return ret_frame_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactPointMatching:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.REPO_NAME = \"facebookresearch/dinov2\"\n",
    "        self.MODEL_NAME = \"dinov2_vitb14\"\n",
    "\n",
    "\n",
    "        self.DEFAULT_SMALLER_EDGE_SIZE = 448\n",
    "        self.IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "        self.IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "        self.model = torch.hub.load(repo_or_dir=self.REPO_NAME, model=self.MODEL_NAME)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.ref_pca_img = None\n",
    "        self.query_pca_img = None\n",
    "        self.ref_tokens = None \n",
    "        self.query_tokens = None\n",
    "        self.ref_mask = None\n",
    "        self.query_mask = None\n",
    "        self.ref_grid_size = None\n",
    "        self.query_grid_size = None\n",
    "        self.ref_scale = None\n",
    "        self.query_scale = None\n",
    "        self.heatmap = None\n",
    "\n",
    "        self.ref_contact_pt = None\n",
    "        self.pred_contact_pt = None\n",
    "    \n",
    "    def zero_pixel(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        val = mean[0] / std[0] + mean[1] / std[1] + mean[2]/std[2]\n",
    "        return -1 * val\n",
    "\n",
    "    def make_transform(self, smaller_edge_size: int) -> transforms.Compose:\n",
    "        IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "        IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "        interpolation_mode = transforms.InterpolationMode.BICUBIC\n",
    "\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(size=smaller_edge_size, interpolation=interpolation_mode, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "        ])\n",
    "\n",
    "\n",
    "    def prepare_image(self, image: Image,\n",
    "                    smaller_edge_size: float,\n",
    "                    patch_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "        transform = self.make_transform(int(smaller_edge_size))\n",
    "        image_tensor = transform(image)\n",
    "        resize_scale = image.width / image_tensor.shape[2]\n",
    "\n",
    "        # Crop image to dimensions that are a multiple of the patch size\n",
    "        height, width = image_tensor.shape[1:] # C x H x W\n",
    "        cropped_width, cropped_height = width - width % patch_size, height - height % patch_size\n",
    "        image_tensor = image_tensor[:, :cropped_height, :cropped_width]\n",
    "\n",
    "        grid_size = (cropped_height // patch_size, cropped_width // patch_size) # h x w (TODO: check)\n",
    "        return image_tensor, grid_size, resize_scale\n",
    "\n",
    "\n",
    "    def make_foreground_mask(self, image_tensor):\n",
    "        mask = torch.sum(image_tensor, dim=0)\n",
    "        threshold = self.zero_pixel()\n",
    "        mask = (torch.abs(mask - threshold) > 0.001).int()\n",
    "        new_size = (mask.size(0) // 14, mask.size(1) // 14)\n",
    "        resized_mask = torch.empty(new_size, dtype=torch.bool)\n",
    "        for i in range(new_size[0]):\n",
    "            for j in range(new_size[1]):\n",
    "                ones = torch.sum(mask[i*14:(i+1)*14, j*14:(j+1)*14])\n",
    "                if ones <= (14 * 14 * 0.8):\n",
    "                    resized_mask[i, j] = False\n",
    "                else:\n",
    "                    resized_mask[i, j] = True\n",
    "        \n",
    "        mask = resized_mask.flatten()\n",
    "        return mask.flatten()\n",
    "\n",
    "    def render_patch_pca(self, ref_image: Image,\n",
    "                        query_image: Image,\n",
    "                        smaller_edge_size: float = 448,\n",
    "                        patch_size: int = 14):\n",
    "        \n",
    "        ref_image_tensor, self.ref_grid_size, self.ref_scale = self.prepare_image(ref_image, smaller_edge_size, patch_size)\n",
    "        query_image_tensor, self.query_grid_size, self.query_scale = self.prepare_image(query_image, smaller_edge_size, patch_size)\n",
    "\n",
    "        print(\"image shape: \", end=\"\")\n",
    "        print(ref_image_tensor.shape, query_image_tensor.shape)\n",
    "\n",
    "        self.ref_mask = self.make_foreground_mask(ref_image_tensor)\n",
    "        self.query_mask = self.make_foreground_mask(query_image_tensor)\n",
    "\n",
    "        print(\"mask shape: \", end=\"\")\n",
    "        print(self.ref_mask.shape, self.query_mask.shape)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            self.ref_tokens = self.model.get_intermediate_layers(ref_image_tensor.unsqueeze(0))[0].squeeze()\n",
    "            self.query_tokens = self.model.get_intermediate_layers(query_image_tensor.unsqueeze(0))[0].squeeze()\n",
    "            # tokens = model(image_batch)\n",
    "\n",
    "        print(\"tokens shape: \", end=\"\")\n",
    "        print(self.ref_tokens.shape, self.query_tokens.shape)\n",
    "\n",
    "        masked_tokens = torch.cat([self.ref_tokens[self.ref_mask], self.query_tokens[self.query_mask]], dim=0)\n",
    "\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(masked_tokens)\n",
    "        projected_ref_tokens = pca.transform(self.ref_tokens)\n",
    "        projected_query_tokens = pca.transform(self.query_tokens)\n",
    "\n",
    "        t = torch.tensor(projected_ref_tokens)\n",
    "        t_min = t.min(dim=0, keepdim=True).values\n",
    "        t_max = t.max(dim=0, keepdim=True).values\n",
    "        normalized_t = (t - t_min) / (t_max - t_min)\n",
    "\n",
    "        array = (normalized_t * 255).byte().numpy()\n",
    "        array[~self.ref_mask] = 0\n",
    "        array = array.reshape(*self.ref_grid_size, 3)\n",
    "        self.ref_pca_img = Image.fromarray(array).resize((ref_image.width, ref_image.height), 0)\n",
    "\n",
    "        t = torch.tensor(projected_query_tokens)\n",
    "        t_min = t.min(dim=0, keepdim=True).values\n",
    "        t_max = t.max(dim=0, keepdim=True).values\n",
    "        normalized_t = (t - t_min) / (t_max - t_min)\n",
    "\n",
    "        array = (normalized_t * 255).byte().numpy()\n",
    "        array[~self.query_mask] = 0\n",
    "        array = array.reshape(*self.query_grid_size, 3)\n",
    "        self.query_pca_img = Image.fromarray(array).resize((query_image.width, query_image.height), 0)\n",
    "\n",
    "    def source_position_to_idx(self, row, col, grid_size, resize_scale):\n",
    "        idx = ((row / resize_scale) // (14)) * grid_size[1] + ((col / resize_scale) // (14))\n",
    "        return int(idx)\n",
    "\n",
    "    def idx_to_source_position(self, idx, grid_size, resize_scale):\n",
    "        row = (idx // grid_size[1])*14*resize_scale + 14 / 2\n",
    "        col = (idx % grid_size[1])*14*resize_scale + 14 / 2\n",
    "        return int(row), int(col)\n",
    "\n",
    "    def closest_embedding(self, ref_embedding, query_embeddings, query_mask):\n",
    "        distances = torch.norm(query_embeddings - ref_embedding, dim=1)\n",
    "        dist_copy = distances.clone()\n",
    "        distances[~query_mask] = float('inf')\n",
    "        return torch.argmin(distances).item(), dist_copy\n",
    "\n",
    "    def generate_heatmap(self, distances, mask, grid_size, image_size):\n",
    "        distances = distances.reshape(grid_size)\n",
    "        mask = mask.reshape(grid_size)\n",
    "        heatmap_np = distances.numpy()\n",
    "        heatmap_np *= -1\n",
    "        heatmap_np = (heatmap_np - np.min(heatmap_np)) / (np.max(heatmap_np) - np.min(heatmap_np))\n",
    "        heatmap_np[~mask] = 0\n",
    "\n",
    "        cmap = plt.get_cmap('jet')\n",
    "        heatmap = cmap(heatmap_np)\n",
    "        heatmap_rgb = (heatmap[:, :, :3] * 255).astype(np.uint8)\n",
    "        resized_heatmap = cv2.resize(heatmap_rgb, (image_size[0], image_size[1]))\n",
    "        \n",
    "        return resized_heatmap\n",
    "\n",
    "    def map_ref_contact_point(self, contact_pt):\n",
    "        self.ref_contact_pt = contact_pt\n",
    "        idx = self.source_position_to_idx(contact_pt[0], contact_pt[1], self.ref_grid_size, self.ref_scale)\n",
    "        matched_idx, distances = self.closest_embedding(self.ref_tokens[idx, :], self.query_tokens, self.query_mask)\n",
    "        self.heatmap = self.generate_heatmap(distances, self.query_mask, self.query_grid_size, self.query_pca_img.size)\n",
    "        row, col = self.idx_to_source_position(matched_idx, self.query_grid_size, self.query_scale)\n",
    "        self.pred_contact_pt = [row, col]\n",
    "        return self.pred_contact_pt\n",
    "    \n",
    "    def visualize(self, query_image_path, ref_image_path):\n",
    "        query_image = Image.open(query_image_path)\n",
    "        ref_image = Image.open(ref_image_path)\n",
    "\n",
    "        draw = ImageDraw.Draw(query_image)\n",
    "        draw.ellipse([self.pred_contact_pt[1]-5, self.pred_contact_pt[0]-5, self.pred_contact_pt[1]+5, self.pred_contact_pt[0]+5], fill=(255, 0, 0))\n",
    "\n",
    "        draw = ImageDraw.Draw(ref_image)\n",
    "        draw.ellipse([self.ref_contact_pt[1]-5, self.ref_contact_pt[0]-5, self.ref_contact_pt[1]+5, self.ref_contact_pt[0]+5], fill=(255, 0, 0))\n",
    "\n",
    "        display(self.ref_pca_img)\n",
    "        display(self.query_pca_img)\n",
    "        display(ref_image)\n",
    "        display(query_image)\n",
    "\n",
    "        query_image = Image.open(query_image_path)\n",
    "        overlay = cv2.addWeighted(np.array(query_image), 0.5, self.heatmap, 0.5, 0)\n",
    "\n",
    "        plt.imshow(overlay)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def main(self, ref_img_path, query_img_path, contact_pts):\n",
    "        query_image = Image.open(query_img_path)\n",
    "        ref_image = Image.open(ref_img_path)\n",
    "\n",
    "        self.render_patch_pca(ref_image, query_image)\n",
    "        \n",
    "        pred_pts = []\n",
    "        for i in range(contact_pts.shape[0]):\n",
    "            pred_pt = self.map_ref_contact_point(contact_pts[i])\n",
    "            pred_pts.append(pred_pt)\n",
    "\n",
    "        return pred_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryGenerator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        self.hf_model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        self.reference_embeddings = []\n",
    "        self.reference_patch_embeddings = []\n",
    "        self.ref_contact_pts = []\n",
    "        self.key_frames = []\n",
    "        self.all_ref_masks = []\n",
    "        self.query_masks = None\n",
    "        self.query_patch_embeddings = None\n",
    "        self.query_embeddings = None\n",
    "        self.matched_query_masks = None\n",
    "        self.matched_query_patch_embeddings = None\n",
    "\n",
    "\n",
    "    def create_mask_images(self, original_image, masks):\n",
    "        original_image_array = np.array(original_image)\n",
    "        all_cropped_imgs = []\n",
    "\n",
    "        for i in range(masks.shape[0]):\n",
    "            mask = masks[i, :, :]\n",
    "            if isinstance(mask, np.ndarray):\n",
    "                mask = torch.from_numpy(mask)\n",
    "            np_mask = mask.unsqueeze(dim=2).numpy()\n",
    "            masked_img = (original_image_array * np_mask).astype(np.uint8)  # Convert to uint8\n",
    "            ys, xs = np.where(mask)\n",
    "            if ys.size == 0 or xs.size == 0:\n",
    "                continue\n",
    "            bbox = np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n",
    "            cropped_image_array = masked_img[bbox[1]:bbox[3]+1, bbox[0]:bbox[2]+1, :]\n",
    "            cropped_image_pil = Image.fromarray(cropped_image_array)\n",
    "            all_cropped_imgs.append(cropped_image_pil)\n",
    "\n",
    "        return all_cropped_imgs\n",
    "\n",
    "    def get_obj_masks(self, img_path, point=\"[[0,0]]\", point_label=\"[0]\", filter=True):\n",
    "        segs = segment(img_path, point_prompt=point, point_label=point_label, filter=filter)\n",
    "        print(segs.shape)\n",
    "        return segs\n",
    "\n",
    "    def get_obj_embeddings(self, img_path, masks):\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        cropped_objs = self.create_mask_images(image, masks)\n",
    "\n",
    "        if len(cropped_objs) != 0:\n",
    "            inputs = self.processor(images=cropped_objs, return_tensors=\"pt\")\n",
    "            outputs = self.hf_model(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            # cls_embeddings = last_hidden_states[:, 0, :].squeeze()\n",
    "            cls_embeddings = last_hidden_states\n",
    "\n",
    "            return cls_embeddings\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_frame_as_image(self, video_path, frame_number, save_path, point=None, radius=5, color=(0, 255, 0), thickness=-1):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        # cv2.circle(frame, point, radius, color, thickness)\n",
    "        cv2.imwrite(save_path, frame)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def closest_pt_to_mask(self, point, mask, crop_transform=False):\n",
    "        mask = mask.squeeze().numpy()\n",
    "        dist_transform, indices = distance_transform_edt(1 - mask, return_indices=True)\n",
    "        closest_point = [indices[1, point[1], point[0]], indices[0, point[1], point[0]]]\n",
    "\n",
    "        if crop_transform:\n",
    "            ys, xs = np.where(mask)\n",
    "            x_min, y_min = np.min(xs), np.min(ys)\n",
    "            closest_point[0] = closest_point[0] - x_min\n",
    "            closest_point[1] = closest_point[1] - y_min\n",
    "\n",
    "        return tuple(closest_point)\n",
    "    \n",
    "    def get_contour_points(self, mask):\n",
    "        mask_np = mask.cpu().numpy().astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        points = np.vstack(contours).squeeze()\n",
    "        return points\n",
    "\n",
    "    def match_points(self, points1, points2, threshold):\n",
    "        matched_points = []\n",
    "        used_indices = set()\n",
    "        for p1 in points1:\n",
    "            min_dist = float('inf')\n",
    "            match_idx = -1\n",
    "            for idx, p2 in enumerate(points2):\n",
    "                if idx in used_indices:\n",
    "                    continue\n",
    "                dist = np.linalg.norm(p1 - p2)\n",
    "                if dist < min_dist and dist < threshold:\n",
    "                    min_dist = dist\n",
    "                    match_idx = idx\n",
    "            if match_idx != -1:\n",
    "                matched_points.append((p1, points2[match_idx]))\n",
    "                used_indices.add(match_idx)\n",
    "        return matched_points\n",
    "    \n",
    "    def find_centroid(self, mask):\n",
    "        non_zero_coords = mask.nonzero(as_tuple=True)\n",
    "        if len(non_zero_coords[0]) == 0:\n",
    "            return None\n",
    "        centroid_y = non_zero_coords[0].float().mean().item()\n",
    "        centroid_x = non_zero_coords[1].float().mean().item()\n",
    "        return int(centroid_y), int(centroid_x)\n",
    "\n",
    "    def visualize_masks_on_frame(self, frame, masks, save_path):\n",
    "        frame_np = np.array(frame)\n",
    "        overlay = frame_np.copy()\n",
    "        centroids = []\n",
    "        colors = [\n",
    "            (255, 0, 0),    # Red\n",
    "            (0, 255, 0),    # Green\n",
    "            (0, 0, 255),    # Blue\n",
    "            (255, 255, 0),  # Yellow\n",
    "            (255, 0, 255),  # Magenta\n",
    "            (0, 255, 255),  # Cyan\n",
    "            (255, 165, 0),  # Orange\n",
    "            (128, 0, 128),  # Purple\n",
    "            (128, 128, 0),  # Olive\n",
    "            (0, 128, 128)   # Teal\n",
    "        ]\n",
    "\n",
    "        for idx, mask in enumerate(masks):\n",
    "            color = colors[idx % len(colors)]\n",
    "            mask_np = mask.cpu().numpy().astype(np.uint8)\n",
    "            colored_mask = np.zeros_like(frame_np)\n",
    "            for j in range(3):\n",
    "                colored_mask[:, :, j] = mask_np * color[j]\n",
    "            overlay = cv2.addWeighted(overlay, 1, colored_mask, 0.5, 0)\n",
    "            centroid = self.find_centroid(mask)\n",
    "            if centroid:\n",
    "                centroids.append(centroid)\n",
    "                cv2.circle(overlay, (centroid[1], centroid[0]), 8, color, -1)\n",
    "\n",
    "        for i in range(1, len(centroids)):\n",
    "            cv2.line(overlay, (centroids[i-1][1], centroids[i-1][0]), (centroids[i][1], centroids[i][0]), colors[i - 1], 4)\n",
    "\n",
    "        overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "        result_image = Image.fromarray(overlay_rgb)\n",
    "        result_image.save(save_path)\n",
    "\n",
    "    def find_interacting_mask(self, masks, mask, unseen=None, distance_threshold=10):\n",
    "        if unseen is None:\n",
    "            unseen = torch.ones(masks.shape[0], dtype=torch.bool)\n",
    "\n",
    "        max_matches = 0\n",
    "        bestMatches = None\n",
    "        interacting_mask = None\n",
    "        mask2 = mask\n",
    "        for i in range(masks.shape[0]):  \n",
    "            mask1 = masks[i]\n",
    "            if unseen[i] == False or torch.equal(mask1, mask2):\n",
    "                unseen[i] = False\n",
    "                continue \n",
    "\n",
    "            points1 = self.get_contour_points(mask1)\n",
    "            points2 = self.get_contour_points(mask2)\n",
    "            \n",
    "            matches = self.match_points(points1, points2, distance_threshold)\n",
    "            \n",
    "            if len(matches) > max_matches:\n",
    "                max_matches = len(matches)\n",
    "                bestMatches = matches\n",
    "                interacting_mask = mask1\n",
    "        \n",
    "        return interacting_mask, bestMatches\n",
    "\n",
    "    def find_closest_mask(self, masks, point1, point2, num_to_match=10):\n",
    "        min_distance = float('inf')\n",
    "        min_idx = -1\n",
    "        closest_mask = None\n",
    "        \n",
    "        for i, mask in enumerate(masks):\n",
    "            contour_points = self.get_contour_points(mask)\n",
    "            distances_point1 = np.linalg.norm(contour_points - np.array(point1), axis=1)\n",
    "            distances_point2 = np.linalg.norm(contour_points - np.array(point2), axis=1)\n",
    "            closest_distances_point1 = np.sort(distances_point1)   # [:num_to_match]\n",
    "            closest_distances_point2 = np.sort(distances_point2)   # [:num_to_match]\n",
    "            total_distance = np.mean(closest_distances_point1) + np.mean(closest_distances_point2)\n",
    "            \n",
    "            if total_distance < min_distance:\n",
    "                min_distance = total_distance\n",
    "                min_idx = i\n",
    "                closest_mask = mask\n",
    "                \n",
    "        return closest_mask, min_idx\n",
    "    \n",
    "    def filter_arms_mask_by_points(self, masks, wrist_point):\n",
    "        idx = -1\n",
    "        area = float('-inf')\n",
    "        for i in range(masks.shape[0]):\n",
    "            if masks[i, wrist_point[0], wrist_point[1]] and torch.sum(masks[i]) > area:\n",
    "                area = torch.sum(masks[i])\n",
    "                idx = i\n",
    "\n",
    "        if idx == -1:\n",
    "            print(\"No mask that contains wrist point.\")\n",
    "            return masks\n",
    "\n",
    "        mask = torch.ones(masks.shape[0], dtype=torch.bool)\n",
    "        mask[idx] = False\n",
    "        filtered_masks = masks[mask]\n",
    "\n",
    "        return filtered_masks\n",
    "    \n",
    "    def find_interaction_chain(self, masks, mask):\n",
    "        interaction_chain = []\n",
    "        contact_pts_chain = []\n",
    "        prev_mask = None\n",
    "        matches = None\n",
    "        mask_ratio_thresh = 0.5\n",
    "        unseen = torch.ones(masks.shape[0], dtype=torch.bool)\n",
    "        while (prev_mask is None or torch.sum(mask) > mask_ratio_thresh * torch.sum(prev_mask)):\n",
    "            prev_mask = mask\n",
    "            interaction_chain.append(prev_mask)\n",
    "            contact_pts_chain.append(matches)\n",
    "            mask, matches = self.find_interacting_mask(masks, prev_mask, unseen)\n",
    "        \n",
    "        return interaction_chain, contact_pts_chain\n",
    "    \n",
    "    def process_ref_video(self, ref_video_path):\n",
    "        frames = get_frame_point(ref_video_path)\n",
    "        for i, points in enumerate(frames):\n",
    "            point = points[0]\n",
    "            frame_num, fingers_point, wrist_point, finger_tips = point\n",
    "            save_path = f\"../images/current_vid_{str(i)}.jpg\"\n",
    "            frame = self.save_frame_as_image(ref_video_path, frame_num, save_path, fingers_point)\n",
    "            # point_input = f\"[[{str(fingers_point[0])},{str(fingers_point[1])}], [{str(wrist_point[0])},{str(wrist_point[1])}]]\"\n",
    "            \n",
    "            masks = self.get_obj_masks(save_path)\n",
    "            filtered_masks = self.filter_arms_mask_by_points(masks, wrist_point[::-1])\n",
    "            self.all_ref_masks.append(masks)\n",
    "\n",
    "            # find closest mask\n",
    "            thumb_pt = [int(finger_tips[0, 0]), int(finger_tips[0, 1])]\n",
    "            index_pt = [int(finger_tips[1, 0]), int(finger_tips[1, 1])]\n",
    "\n",
    "            closest_mask, closest_idx = self.find_closest_mask(filtered_masks, thumb_pt, index_pt)\n",
    "\n",
    "            # compute closest finger tips to mask\n",
    "\n",
    "            thumb_pt = self.closest_pt_to_mask(thumb_pt, closest_mask, crop_transform=True)\n",
    "            index_pt = self.closest_pt_to_mask(index_pt, closest_mask, crop_transform=True)\n",
    "\n",
    "            self.ref_contact_pts.append([[thumb_pt[0], thumb_pt[1]], [index_pt[0], index_pt[1]]])\n",
    "\n",
    "            # embeddings\n",
    "            embedding = self.get_obj_embeddings(save_path, closest_mask.unsqueeze(dim=0))\n",
    "            cls_embedding = embedding[:, 0, :].squeeze()\n",
    "            self.reference_embeddings.append(cls_embedding)\n",
    "            self.reference_patch_embeddings.append(embedding[:, 1:, :].squeeze())\n",
    "\n",
    "            # find interaction chain\n",
    "            interaction_chain, contact_pts_chain = self.find_interaction_chain(filtered_masks, closest_mask)\n",
    "\n",
    "\n",
    "            output_path = f\"./output/current_vid_{str(i)}.jpg\"\n",
    "            output_path2 = f\"./output/current_interaction_{str(i)}.jpg\"\n",
    "\n",
    "            frame = cv2.imread(save_path)\n",
    "\n",
    "            self.visualize_masks_on_frame(frame, interaction_chain, output_path2)\n",
    "            print(f\"Written interaction image to {output_path2}\")\n",
    "\n",
    "\n",
    "            masked_frame = self.create_mask_images(frame, closest_mask.unsqueeze(dim=0))[0]\n",
    "            masked_frame_np = np.array(masked_frame)\n",
    "\n",
    "            # saving image of only closest mask (cropped)\n",
    "            cv2.imwrite(output_path, masked_frame_np)\n",
    "            print(f\"Written closest mask image to {output_path}\")\n",
    "\n",
    "\n",
    "        self.ref_contact_pts = torch.tensor(self.ref_contact_pts)\n",
    "        self.reference_embeddings = torch.stack(self.reference_embeddings)\n",
    "        self.reference_patch_embeddings = torch.stack(self.reference_patch_embeddings)\n",
    "    \n",
    "    def process_query_image(self, query_img_path):\n",
    "        self.query_masks = self.get_obj_masks(query_img_path, filter=False)\n",
    "        query_embeddings = self.get_obj_embeddings(query_img_path, self.query_masks)\n",
    "        self.query_patch_embeddings = query_embeddings[:, 1:, :]\n",
    "        self.query_embeddings = query_embeddings[:, 0, :].squeeze()    \n",
    "    \n",
    "    def sim_matrix(self, a, b, eps=1e-8):\n",
    "        a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "        a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "        b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "        sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "        return sim_mt\n",
    "    \n",
    "    def match_ref_and_query(self, query_path):\n",
    "        similarities = self.sim_matrix(self.query_embeddings, self.reference_embeddings)\n",
    "        matched_query_masks_idx = torch.argmax(similarities, dim=0)\n",
    "        self.matched_query_masks = self.query_masks[matched_query_masks_idx, :, :]\n",
    "        self.matched_query_patch_embeddings = self.query_patch_embeddings[matched_query_masks_idx, :, :]\n",
    "\n",
    "        query_img = cv2.imread(query_path)\n",
    "        for i in range(self.matched_query_masks.shape[0]):\n",
    "            masked_frame = self.create_mask_images(query_img, self.matched_query_masks[i:i+1])[0]\n",
    "            masked_frame_np = np.array(masked_frame)\n",
    "\n",
    "            output_path = f\"./output/query_img_{str(i)}.jpg\"\n",
    "            cv2.imwrite(output_path, masked_frame_np)\n",
    "            print(f\"Written matching query image to {output_path}\")\n",
    "\n",
    "    \n",
    "    def main(self, ref_path, query_img_path):\n",
    "        self.process_ref_video(ref_path)\n",
    "        self.process_query_image(query_img_path)\n",
    "        self.match_ref_and_query(query_img_path)\n",
    "        contact_matcher = ContactPointMatching()\n",
    "\n",
    "        pred_pts = []\n",
    "        num_frames = len(self.key_frames)\n",
    "        for i in range(num_frames):\n",
    "            ref_frame_path = f\"./output/current_vid_{str(i)}.jpg\"\n",
    "            query_obj_path = f\"./output/query_img_{str(i)}.jpg\"\n",
    "\n",
    "            contact_pts = self.ref_contact_pts[i]\n",
    "            pred_pts.append(contact_matcher.main(ref_frame_path, query_obj_path, contact_pts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/511 [00:00<?, ?it/s]c:\\Users\\arshs\\miniconda3\\envs\\dlproj\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "100%|██████████| 511/511 [00:29<00:00, 17.16it/s]\n",
      "\n",
      "0: 576x1024 37 objects, 4637.7ms\n",
      "Speed: 15.0ms preprocess, 4637.7ms inference, 917.3ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1080, 1920])\n",
      "Written interaction image to ./output/current_interaction_0.jpg\n",
      "Written closest mask image to ./output/current_vid_0.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 39 objects, 5009.9ms\n",
      "Speed: 14.0ms preprocess, 5009.9ms inference, 1168.7ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 1080, 1920])\n",
      "Written interaction image to ./output/current_interaction_1.jpg\n",
      "Written closest mask image to ./output/current_vid_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 30 objects, 5356.2ms\n",
      "Speed: 15.2ms preprocess, 5356.2ms inference, 849.4ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1080, 1920])\n",
      "Written interaction image to ./output/current_interaction_2.jpg\n",
      "Written closest mask image to ./output/current_vid_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 34 objects, 5494.3ms\n",
      "Speed: 67.5ms preprocess, 5494.3ms inference, 1015.7ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1080, 1920])\n",
      "Written interaction image to ./output/current_interaction_3.jpg\n",
      "Written closest mask image to ./output/current_vid_3.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 42 objects, 5195.5ms\n",
      "Speed: 16.0ms preprocess, 5195.5ms inference, 1002.3ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 1080, 1920])\n",
      "Written interaction image to ./output/current_interaction_4.jpg\n",
      "Written closest mask image to ./output/current_vid_4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 43 objects, 4695.4ms\n",
      "Speed: 14.4ms preprocess, 4695.4ms inference, 1023.9ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 1080, 1920])\n",
      "Written interaction image to ./output/current_interaction_5.jpg\n",
      "Written closest mask image to ./output/current_vid_5.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 48 objects, 4829.7ms\n",
      "Speed: 16.3ms preprocess, 4829.7ms inference, 1072.1ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1080, 1920])\n",
      "Written interaction image to ./output/current_interaction_6.jpg\n",
      "Written closest mask image to ./output/current_vid_6.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 45 objects, 4891.0ms\n",
      "Speed: 15.0ms preprocess, 4891.0ms inference, 976.2ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 1080, 1920])\n",
      "Written interaction image to ./output/current_interaction_7.jpg\n",
      "Written closest mask image to ./output/current_vid_7.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 544x1024 28 objects, 4602.2ms\n",
      "Speed: 14.1ms preprocess, 4602.2ms inference, 604.4ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 970, 1834])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\arshs/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\arshs/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\arshs/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\arshs/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "tg = TrajectoryGenerator()\n",
    "tg.main(\"../videos/IMG_3288.MOV\", \"../images/query_img.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
