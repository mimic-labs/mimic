{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Tuple\n",
    "from PIL import ImageDraw\n",
    "\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)\n",
    "%cd {HOME}/FastSAM\n",
    "\n",
    "from Inference import segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_point(videoPath, out=False):\n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False,\n",
    "                        max_num_hands=2,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    centers = [[], []]\n",
    "    wrists = [[], []]\n",
    "\n",
    "    # Function to calculate standard deviation of distances\n",
    "    def calculate_stdev(coordinates):\n",
    "        distances = np.linalg.norm(coordinates - np.mean(coordinates, axis=0), axis=1)\n",
    "        stdev = np.std(distances)\n",
    "        return stdev\n",
    "\n",
    "    # Function to draw a circle on the frame\n",
    "    def draw_circle(frame, center, RED=1):\n",
    "        cv2.circle(frame, center, 20, (0, 255*RED, 255*(1-RED)), -1)\n",
    "        \n",
    "    def extract_hand_data(frame):\n",
    "        results = hands.process(frame)\n",
    "        # print(results.multi_hand_world_landmarks)\n",
    "\n",
    "        if results.multi_hand_world_landmarks:\n",
    "            for hand in results.multi_handedness:\n",
    "                # Get a constant index for the detected hand (0 or 1). If only 1 hand is detected, default to index = 0.\n",
    "                hand_idx = hand.classification[0].index\n",
    "                try:\n",
    "                    hand_landmarks = results.multi_hand_landmarks[hand_idx]\n",
    "                except:\n",
    "                    hand_idx = 0\n",
    "                    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "                \n",
    "                # Get key points on palm\n",
    "                # palm_points = np.asarray([[hand_landmarks.landmark[12].x, hand_landmarks.landmark[12].y, hand_landmarks.landmark[12].z], \n",
    "                #                         [hand_landmarks.landmark[16].x, hand_landmarks.landmark[16].y, hand_landmarks.landmark[16].z], \n",
    "                #                         [hand_landmarks.landmark[20].x, hand_landmarks.landmark[20].y, hand_landmarks.landmark[20].z],\n",
    "                #                         [hand_landmarks.landmark[4].x, hand_landmarks.landmark[4].y, hand_landmarks.landmark[4].z],\n",
    "                #                         [hand_landmarks.landmark[8].x, hand_landmarks.landmark[8].y, hand_landmarks.landmark[8].z]])\n",
    "                palm_points = np.asarray([\n",
    "                        [hand_landmarks.landmark[4].x, hand_landmarks.landmark[4].y, hand_landmarks.landmark[4].z],\n",
    "                        [hand_landmarks.landmark[8].x, hand_landmarks.landmark[8].y, hand_landmarks.landmark[8].z]])\n",
    "\n",
    "                # Get palm orientation by calculating normal vector of palm plane\n",
    "                # normal_vector = np.cross(palm_points[2] - palm_points[0], palm_points[1] - palm_points[2])\n",
    "                # normal_vector /= np.linalg.norm(normal_vector)\n",
    "                # orientations[hand_idx].append(normal_vector)\n",
    "\n",
    "                # Get hand center\n",
    "                palm_points_mean = np.mean(palm_points, axis=0)\n",
    "                center_x = int(palm_points_mean[0] * frame.shape[1])\n",
    "                center_y = int(palm_points_mean[1] * frame.shape[0])\n",
    "                centers[hand_idx].append((center_x, center_y))\n",
    "            \n",
    "                wrist_center_x = int(hand_landmarks.landmark[0].x * frame.shape[1])\n",
    "                wrist_center_y = int(hand_landmarks.landmark[0].y * frame.shape[0])\n",
    "                wrists[hand_idx].append((wrist_center_x, wrist_center_y))\n",
    "                # cv2.circle(frame, (center_x, center_y), 3, (255, 0, 0))\n",
    "            # for x in range(2):\n",
    "            #     # Draw current & past hand centers on existing frame\n",
    "            #     cv2.polylines(frame, [np.array(centers[x])], False, (0,255*(1-x),255*x), 3)\n",
    "            \n",
    "        return frame\n",
    "\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    if out:\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(f\"out.mp4\"),\n",
    "            cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "            fps, (frame_width, frame_height)\n",
    "        )\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "\n",
    "    # Parameters\n",
    "    window_size = 20\n",
    "    stdev_threshold = 5\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    # Initialize variables\n",
    "    centers_window = [[],[]]\n",
    "\n",
    "    on = [False, False]\n",
    "    ret_frame_point = [] #frame, hand1_point, hand2_point\n",
    "    for x in tqdm(range(num_frames)):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get hand center coordinates\n",
    "        frame = extract_hand_data(frame)\n",
    "        frame_level = []\n",
    "        # print(centers)\n",
    "        for h in range(2):\n",
    "            hand_center = centers[h][-1] if len(centers[h]) > 0 else None\n",
    "            # print(hand_center)\n",
    "\n",
    "            if hand_center is not None:\n",
    "                # Add hand center to the window\n",
    "                centers_window[h].append(hand_center)\n",
    "\n",
    "                # Keep the window size limited to the last 20 frames\n",
    "                if len(centers_window[h]) > window_size:\n",
    "                    centers_window[h].pop(0)\n",
    "\n",
    "                # Calculate standard deviation of distances\n",
    "                if len(centers_window[h]) == window_size:\n",
    "                    stdev = calculate_stdev(np.array(centers_window[h]))\n",
    "\n",
    "                    # Check if stdev is below the threshold\n",
    "                    if stdev < stdev_threshold:\n",
    "                        if not on[h]:\n",
    "                            on[h]=True\n",
    "                            # Draw a circle on the frame at the average center\n",
    "                            average_center = tuple(centers_window[h][-1]) #tuple(np.mean(centers_window, axis=0).astype(int))\n",
    "                            frame_level.append((x,average_center, wrists[h][-1]))\n",
    "                            draw_circle(frame, average_center, RED=h)\n",
    "                    else:\n",
    "                        on[h]=False\n",
    "        if frame_level:\n",
    "            ret_frame_point.append(frame_level)\n",
    "\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Display the frame\n",
    "        if out:\n",
    "            out.write(frame)\n",
    "\n",
    "        # Exit when 'q' key is pressed\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "\n",
    "    # Release video capture\n",
    "    if out:\n",
    "        out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return ret_frame_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryGenerator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        self.hf_model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        self.reference_embeddings = []\n",
    "        self.reference_patch_embeddings = []\n",
    "        self.key_frames = []\n",
    "        self.all_ref_masks = []\n",
    "        self.query_masks = None\n",
    "        self.query_patch_embeddings = None\n",
    "        self.query_embeddings = None\n",
    "        self.matched_query_masks = None\n",
    "        self.matched_query_patch_embeddings = None\n",
    "\n",
    "\n",
    "    def create_mask_images(self, original_image, masks):\n",
    "        original_image_array = np.array(original_image)\n",
    "        all_cropped_imgs = []\n",
    "\n",
    "        for i in range(masks.shape[0]):\n",
    "            mask = masks[i, :, :]\n",
    "            if isinstance(mask, np.ndarray):\n",
    "                mask = torch.from_numpy(mask)\n",
    "            np_mask = mask.unsqueeze(dim=2).numpy()\n",
    "            masked_img = (original_image_array * np_mask).astype(np.uint8)  # Convert to uint8\n",
    "            ys, xs = np.where(mask)\n",
    "            if ys.size == 0 or xs.size == 0:\n",
    "                continue\n",
    "            bbox = np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n",
    "            cropped_image_array = masked_img[bbox[1]:bbox[3]+1, bbox[0]:bbox[2]+1, :]\n",
    "            cropped_image_pil = Image.fromarray(cropped_image_array)\n",
    "            all_cropped_imgs.append(cropped_image_pil)\n",
    "\n",
    "        return all_cropped_imgs\n",
    "\n",
    "    def get_obj_masks(self, img_path, point=\"[[0,0]]\", point_label=\"[0]\", filter=True):\n",
    "        segs = segment(img_path, point_prompt=point, point_label=point_label, filter=filter)\n",
    "        print(segs.shape)\n",
    "        return segs\n",
    "\n",
    "    def get_obj_embeddings(self, img_path, masks):\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        cropped_objs = self.create_mask_images(image, masks)\n",
    "\n",
    "        if len(cropped_objs) != 0:\n",
    "            inputs = self.processor(images=cropped_objs, return_tensors=\"pt\")\n",
    "            outputs = self.hf_model(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            # cls_embeddings = last_hidden_states[:, 0, :].squeeze()\n",
    "            cls_embeddings = last_hidden_states\n",
    "\n",
    "            return cls_embeddings\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_frame_as_image(self, video_path, frame_number, save_path, point=None, radius=5, color=(0, 255, 0), thickness=-1):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number-30)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        # cv2.circle(frame, point, radius, color, thickness)\n",
    "        cv2.imwrite(save_path, frame)\n",
    "\n",
    "        return frame\n",
    "    \n",
    "    def sort_masks_by_interaction(self, finger_point, masks):\n",
    "        interactions = []\n",
    "\n",
    "        for mask in masks:\n",
    "            ys, xs = np.where(mask)\n",
    "            x_min, y_min, x_max, y_max = np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n",
    "            center = torch.tensor([(x_min + x_max) / 2, (y_min + y_max) / 2])\n",
    "            distance = torch.norm(finger_point - center)\n",
    "            mask_area = torch.sum(mask)\n",
    "\n",
    "            distance_weight = 0.995\n",
    "            area_weight = 0\n",
    "            # print(distance, mask_area)\n",
    "            weighted_sum = distance_weight * distance + area_weight * mask_area\n",
    "            interactions.append((weighted_sum, mask))\n",
    "\n",
    "        sorted_interactions = sorted(interactions, key=lambda x: x[0])\n",
    "        sorted_masks = torch.stack([mask for _, mask in sorted_interactions])\n",
    "        return sorted_masks\n",
    "    \n",
    "    def process_ref_video(self, ref_video_path):\n",
    "        frames = get_frame_point(ref_video_path)\n",
    "        for i, points in enumerate(frames):\n",
    "            point = points[0]\n",
    "            frame_num, fingers_point, wrist_point = point\n",
    "            save_path = f\"../images/current_vid_{str(i)}.jpg\"\n",
    "            frame = self.save_frame_as_image(ref_video_path, frame_num, save_path, fingers_point)\n",
    "            # point_input = f\"[[{str(fingers_point[0])},{str(fingers_point[1])}], [{str(wrist_point[0])},{str(wrist_point[1])}]]\"\n",
    "            \n",
    "            self.all_ref_masks.append(self.get_obj_masks(save_path))\n",
    "\n",
    "            # find closest mask\n",
    "            finger_point_tensor = torch.tensor([fingers_point[0], fingers_point[1]])\n",
    "            sorted_masks = self.sort_masks_by_interaction(finger_point_tensor, self.all_ref_masks[i])\n",
    "\n",
    "            embedding = self.get_obj_embeddings(save_path, sorted_masks[0:1])\n",
    "            cls_embedding = embedding[:, 0, :].squeeze()\n",
    "\n",
    "            self.reference_embeddings.append(cls_embedding)\n",
    "            self.reference_patch_embeddings.append(embedding[:, 1:, :].squeeze())\n",
    "\n",
    "            # output frame of two closest masks unioned \n",
    "            # frame = cv2.imread(save_path)\n",
    "            # union_mask = torch.logical_or(sorted_masks[0], sorted_masks[0])\n",
    "            # mask_np = union_mask.cpu().numpy().astype(np.uint8) * 255\n",
    "            # masked_frame = cv2.bitwise_and(frame, frame, mask=mask_np)\n",
    "\n",
    "            masked_frame = self.create_mask_images(frame, sorted_masks[0:1])[0]\n",
    "            masked_frame_np = np.array(masked_frame)\n",
    "\n",
    "            # saving image of only closest mask (cropped)\n",
    "            output_path = f\"./output/current_vid_{str(i)}.jpg\"\n",
    "            cv2.imwrite(output_path, masked_frame_np)\n",
    "\n",
    "\n",
    "        self.reference_embeddings = torch.stack(self.reference_embeddings)\n",
    "        self.reference_patch_embeddings = torch.stack(self.reference_patch_embeddings)\n",
    "    \n",
    "    def process_query_image(self, query_img_path):\n",
    "        self.query_masks = self.get_obj_masks(query_img_path, filter=False)\n",
    "        query_embeddings = self.get_obj_embeddings(query_img_path, self.query_masks)\n",
    "        self.query_patch_embeddings = query_embeddings[:, 1:, :]\n",
    "        self.query_embeddings = query_embeddings[:, 0, :].squeeze()    \n",
    "    \n",
    "    def sim_matrix(self, a, b, eps=1e-8):\n",
    "        a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "        a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "        b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "        sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "        return sim_mt\n",
    "    \n",
    "    def match_ref_and_query(self, query_path):\n",
    "        similarities = self.sim_matrix(self.query_embeddings, self.reference_embeddings)\n",
    "        matched_query_masks_idx = torch.argmax(similarities, dim=0)\n",
    "        self.matched_query_masks = self.query_masks[matched_query_masks_idx, :, :]\n",
    "        self.matched_query_patch_embeddings = self.query_patch_embeddings[matched_query_masks_idx, :, :]\n",
    "\n",
    "        query_img = cv2.imread(query_path)\n",
    "        for i in range(self.matched_query_masks.shape[0]):\n",
    "            masked_frame = self.create_mask_images(query_img, self.matched_query_masks[i:i+1])[0]\n",
    "            masked_frame_np = np.array(masked_frame)\n",
    "\n",
    "            output_path = f\"./output/query_img_{str(i)}.jpg\"\n",
    "            cv2.imwrite(output_path, masked_frame_np)\n",
    "    \n",
    "    def main(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactPointMatching:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.REPO_NAME = \"facebookresearch/dinov2\"\n",
    "        self.MODEL_NAME = \"dinov2_vitb14\"\n",
    "\n",
    "\n",
    "        self.DEFAULT_SMALLER_EDGE_SIZE = 448\n",
    "        self.IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "        self.IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "        self.model = torch.hub.load(repo_or_dir=self.REPO_NAME, model=self.MODEL_NAME)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.ref_pca_img = None\n",
    "        self.query_pca_img = None\n",
    "        self.ref_tokens = None \n",
    "        self.query_tokens = None\n",
    "        self.ref_mask = None\n",
    "        self.query_mask = None\n",
    "        self.ref_grid_size = None\n",
    "        self.query_grid_size = None\n",
    "        self.ref_scale = None\n",
    "        self.query_scale = None\n",
    "        self.heatmap = None\n",
    "\n",
    "        self.ref_contact_pt = None\n",
    "        self.pred_contact_pt = None\n",
    "    \n",
    "    def zero_pixel(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        val = mean[0] / std[0] + mean[1] / std[1] + mean[2]/std[2]\n",
    "        return -1 * val\n",
    "\n",
    "    def make_transform(self, smaller_edge_size: int) -> transforms.Compose:\n",
    "        IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "        IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "        interpolation_mode = transforms.InterpolationMode.BICUBIC\n",
    "\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(size=smaller_edge_size, interpolation=interpolation_mode, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "        ])\n",
    "\n",
    "\n",
    "    def prepare_image(self, image: Image,\n",
    "                    smaller_edge_size: float,\n",
    "                    patch_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "        transform = self.make_transform(int(smaller_edge_size))\n",
    "        image_tensor = transform(image)\n",
    "        resize_scale = image.width / image_tensor.shape[2]\n",
    "\n",
    "        # Crop image to dimensions that are a multiple of the patch size\n",
    "        height, width = image_tensor.shape[1:] # C x H x W\n",
    "        cropped_width, cropped_height = width - width % patch_size, height - height % patch_size\n",
    "        image_tensor = image_tensor[:, :cropped_height, :cropped_width]\n",
    "\n",
    "        grid_size = (cropped_height // patch_size, cropped_width // patch_size) # h x w (TODO: check)\n",
    "        return image_tensor, grid_size, resize_scale\n",
    "\n",
    "\n",
    "    def make_foreground_mask(self, image_tensor):\n",
    "        mask = torch.sum(image_tensor, dim=0)\n",
    "        threshold = self.zero_pixel()\n",
    "        mask = (torch.abs(mask - threshold) > 0.001).int()\n",
    "        new_size = (mask.size(0) // 14, mask.size(1) // 14)\n",
    "        resized_mask = torch.empty(new_size, dtype=torch.bool)\n",
    "        for i in range(new_size[0]):\n",
    "            for j in range(new_size[1]):\n",
    "                ones = torch.sum(mask[i*14:(i+1)*14, j*14:(j+1)*14])\n",
    "                if ones <= (14 * 14 * 0.8):\n",
    "                    resized_mask[i, j] = False\n",
    "                else:\n",
    "                    resized_mask[i, j] = True\n",
    "        \n",
    "        mask = resized_mask.flatten()\n",
    "        return mask.flatten()\n",
    "\n",
    "    def render_patch_pca(self, ref_image: Image,\n",
    "                        query_image: Image,\n",
    "                        smaller_edge_size: float = 448,\n",
    "                        patch_size: int = 14):\n",
    "        \n",
    "        ref_image_tensor, self.ref_grid_size, self.ref_scale = self.prepare_image(ref_image, smaller_edge_size, patch_size)\n",
    "        query_image_tensor, self.query_grid_size, self.query_scale = self.prepare_image(query_image, smaller_edge_size, patch_size)\n",
    "\n",
    "        print(\"image shape: \", end=\"\")\n",
    "        print(ref_image_tensor.shape, query_image_tensor.shape)\n",
    "\n",
    "        self.ref_mask = self.make_foreground_mask(ref_image_tensor)\n",
    "        self.query_mask = self.make_foreground_mask(query_image_tensor)\n",
    "\n",
    "        print(\"mask shape: \", end=\"\")\n",
    "        print(self.ref_mask.shape, self.query_mask.shape)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            self.ref_tokens = self.model.get_intermediate_layers(ref_image_tensor.unsqueeze(0))[0].squeeze()\n",
    "            self.query_tokens = self.model.get_intermediate_layers(query_image_tensor.unsqueeze(0))[0].squeeze()\n",
    "            # tokens = model(image_batch)\n",
    "\n",
    "        print(\"tokens shape: \", end=\"\")\n",
    "        print(self.ref_tokens.shape, self.query_tokens.shape)\n",
    "\n",
    "        masked_tokens = torch.cat([self.ref_tokens[self.ref_mask], self.query_tokens[self.query_mask]], dim=0)\n",
    "\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(masked_tokens)\n",
    "        projected_ref_tokens = pca.transform(self.ref_tokens)\n",
    "        projected_query_tokens = pca.transform(self.query_tokens)\n",
    "\n",
    "        t = torch.tensor(projected_ref_tokens)\n",
    "        t_min = t.min(dim=0, keepdim=True).values\n",
    "        t_max = t.max(dim=0, keepdim=True).values\n",
    "        normalized_t = (t - t_min) / (t_max - t_min)\n",
    "\n",
    "        array = (normalized_t * 255).byte().numpy()\n",
    "        array[~self.ref_mask] = 0\n",
    "        array = array.reshape(*self.ref_grid_size, 3)\n",
    "        self.ref_pca_img = Image.fromarray(array).resize((ref_image.width, ref_image.height), 0)\n",
    "\n",
    "        t = torch.tensor(projected_query_tokens)\n",
    "        t_min = t.min(dim=0, keepdim=True).values\n",
    "        t_max = t.max(dim=0, keepdim=True).values\n",
    "        normalized_t = (t - t_min) / (t_max - t_min)\n",
    "\n",
    "        array = (normalized_t * 255).byte().numpy()\n",
    "        array[~self.query_mask] = 0\n",
    "        array = array.reshape(*self.query_grid_size, 3)\n",
    "        self.query_pca_img = Image.fromarray(array).resize((query_image.width, query_image.height), 0)\n",
    "\n",
    "    def source_position_to_idx(self, row, col, grid_size, resize_scale):\n",
    "        idx = ((row / resize_scale) // (14)) * grid_size[1] + ((col / resize_scale) // (14))\n",
    "        return int(idx)\n",
    "\n",
    "    def idx_to_source_position(self, idx, grid_size, resize_scale):\n",
    "        row = (idx // grid_size[1])*14*resize_scale + 14 / 2\n",
    "        col = (idx % grid_size[1])*14*resize_scale + 14 / 2\n",
    "        return int(row), int(col)\n",
    "\n",
    "    def closest_embedding(self, ref_embedding, query_embeddings, query_mask):\n",
    "        distances = torch.norm(query_embeddings - ref_embedding, dim=1)\n",
    "        dist_copy = distances.clone()\n",
    "        distances[~query_mask] = float('inf')\n",
    "        return torch.argmin(distances).item(), dist_copy\n",
    "\n",
    "    def generate_heatmap(self, distances, mask, grid_size, image_size):\n",
    "        distances = distances.reshape(grid_size)\n",
    "        mask = mask.reshape(grid_size)\n",
    "        heatmap_np = distances.numpy()\n",
    "        heatmap_np *= -1\n",
    "        heatmap_np = (heatmap_np - np.min(heatmap_np)) / (np.max(heatmap_np) - np.min(heatmap_np))\n",
    "        heatmap_np[~mask] = 0\n",
    "\n",
    "        cmap = plt.get_cmap('jet')\n",
    "        heatmap = cmap(heatmap_np)\n",
    "        heatmap_rgb = (heatmap[:, :, :3] * 255).astype(np.uint8)\n",
    "        resized_heatmap = cv2.resize(heatmap_rgb, (image_size[0], image_size[1]))\n",
    "        \n",
    "        return resized_heatmap\n",
    "\n",
    "    def map_ref_contact_point(self, contact_pt):\n",
    "        self.ref_contact_pt = contact_pt\n",
    "        idx = self.source_position_to_idx(contact_pt[0], contact_pt[1], self.ref_grid_size, self.ref_scale)\n",
    "        matched_idx, distances = self.closest_embedding(self.ref_tokens[idx, :], self.query_tokens, self.query_mask)\n",
    "        self.heatmap = self.generate_heatmap(distances, self.query_mask, self.query_grid_size, self.query_pca_img.size)\n",
    "        row, col = self.idx_to_source_position(matched_idx, self.query_grid_size, self.query_scale)\n",
    "        self.pred_contact_pt = np.array([row, col])\n",
    "        return self.pred_contact_pt\n",
    "    \n",
    "    def visualize(self, query_image_path, ref_image_path):\n",
    "        query_image = Image.open(query_image_path)\n",
    "        ref_image = Image.open(ref_image_path)\n",
    "\n",
    "        draw = ImageDraw.Draw(query_image)\n",
    "        draw.ellipse([self.pred_contact_pt[1]-5, self.pred_contact_pt[0]-5, self.pred_contact_pt[1]+5, self.pred_contact_pt[0]+5], fill=(255, 0, 0))\n",
    "\n",
    "        draw = ImageDraw.Draw(ref_image)\n",
    "        draw.ellipse([self.ref_contact_pt[1]-5, self.ref_contact_pt[0]-5, self.ref_contact_pt[1]+5, self.ref_contact_pt[0]+5], fill=(255, 0, 0))\n",
    "\n",
    "        display(self.ref_pca_img)\n",
    "        display(self.query_pca_img)\n",
    "        display(ref_image)\n",
    "        display(query_image)\n",
    "\n",
    "        query_image = Image.open(query_image_path)\n",
    "        overlay = cv2.addWeighted(np.array(query_image), 0.5, self.heatmap, 0.5, 0)\n",
    "\n",
    "        plt.imshow(overlay)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def main(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
